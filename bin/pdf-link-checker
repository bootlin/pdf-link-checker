#!/usr/bin/env python3
#
# pdf-link-checker: report broken hyperlinks in PDF documents
#
# About page: https://bootlin.com/blog/pdf-link-checker
# Required packages: python-pdfminer
# Recommended packages: pdfminer-data (encoding data needed to read
# some PDF documents in CJK (Chinese, Japanese, Korean) languages.
#
# Copyright (C) 2012-2016 Bootlin
# Author: Ezequiel Garcia <ezequiel.garcia at bootlin com>
#
#   URL checking based on 'coool' script
#   Copyright (C) 2005-2013 Bootlin
#   Author: Michael Opdenacker <michael at bootlin com>
#   https://github.com/bootlin/odf-tools
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
# WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
# NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
# NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
# USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
# ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
# THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
# You should have received a copy of the  GNU General Public License along
# with this program; if not, write  to the Free Software Foundation, Inc.,
# 675 Mass Ave, Cambridge, MA 02139, USA.
#########################################################3

import os
import re
import sys
import time
import socket
import threading
import logging as log
import html
import urllib.request
import http.client
from urllib.parse import urlparse
from pdfminer.pdfparser import PDFParser
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdftypes import PDFStream
from optparse import OptionParser


##########################################################
# Common routines
##########################################################

def touch(fname):
    # Implementation of the touch command
    if os.path.exists(fname):
        os.utime(fname, None)
    else:
        open(fname, 'w').close()


SLASHDRV_PAT = re.compile('^/([A-Z]:)')


def url_fix(url):
    # This snippet idea has been taken from 'Werkzeug' project.
    # See: http://werkzeug.pocoo.org/
    #
    # Sometimes you get an URL by a user that just isn't a real
    # URL because it contains unsafe characters like ' ' and so on.
    # This function can fix some of the problems in a similar way browsers
    # handle data entered by the user.

    # Another thing we want to do is rewrite Windows file URLs
    # that urllib won't know how to handle otherwise.

    scheme, netloc, path, qs, anchor = urllib.parse.urlsplit(url)

    if scheme == 'file':
        if netloc:
            if netloc != 'localhost':
                #log.debug('Hostname ' + str(netloc) + ' for ' + str(url))
                path = '//' + netloc + path
                netloc = 'localhost'
        elif True:
            path = SLASHDRV_PAT.sub('\\1', path)
            #log.debug('New path: ' + path + ' for ' + str(url))
    else:
        path = urllib.parse.quote(path, '/%')
        qs = urllib.parse.quote_plus(qs, ':&=')

    return urllib.parse.urlunsplit((scheme, netloc, path, qs, anchor))


##########################################################
# URL checking
##########################################################

def check_http_url(url, timeout):

    request = urllib.request.Request(url)

    # Add a browser-like User-Agent.
    # Some sites (like wikipedia) don't seem to accept requests
    # with the default urllib User-Agent
    request.add_header('User-Agent',
                       'Mozilla/5.0 (X11; U; Linux i686; en-US;'
                       'rv 1.7.8) Gecko/20050511')

    try:
        urllib.request.urlopen(request)
    except urllib.error.HTTPError as why:
        return (False, why)
    except urllib.error.URLError as why:
        return (False, why)
    except http.client.BadStatusLine as why:
        return (False, why)
    except socket.timeout as why:
        return (False, why)
    except:
        return (False, 'unknown')
    return (True, None)


def check_ftp_url(url):
    # urllib doesn't raise any exception when a
    # ftp url for an invalid file is given
    # Only invalid domains are reported
    # That's why we are using urllib.urlretrieve

    try:
        tmpfile = urllib.request.urlretrieve(url)[0]
    except IOError as why:
        return (False, why)
    else:
        # With a non-existing file on a ftp URL,
        # we get a zero size output file
        # Even if a real zero size file exists,
        # it's worth highlighting anyway
        # (no point in making an hyperlink to it)
        if os.path.getsize(tmpfile) == 0:
            os.remove(tmpfile)
            return (False, 'Non existing or empty file')
    return (True, None)


def check_non_http_or_ftp_url(url):

    request = urllib.request.Request(url)
    request.get_method = lambda: 'HEAD'

    try:
        urllib.request.urlopen(request)
        return (True, None)

    except IOError as why:
        if 'is a directory' in str(why).lower():
            # returning a directory is perfectly fine
            return (True, None)
        elif 'mailto' in str(why).lower():
            # link to an e-mail address
            return (True, None)
        log.debug('Cannot open file URL: {} '.format(why))
        return (False, why)


def check_url(url, timeout):

    protocol = urlparse(url)[0]

    # TODO: support relative links;
    # PDF also has 'internal links',
    # we should check them, too.

    log.debug('Checking link: {}'.format(url))

    for i in range(2):

        res = [True, 'skipped']

        if protocol == 'http' or protocol == 'https':
            res = check_http_url(url, timeout)
        elif protocol == 'ftp':
            res = check_ftp_url(url)
        elif protocol:
            res = check_non_http_or_ftp_url(url)
        else:
            # no protocol could be found, and we don't support
            # relative links yet
            log.debug('Skipping unsupported link {}'.format(url))
            return (False, 'Relative')

        # If we get a False result,
        # we'll retry (once) with a 'fixed' url
        if res[0] is False and i == 0:
            fixed = url_fix(url)
            # Only do the check with fixed url if it's different
            # from the original one
            if fixed == url:
                break
            url = fixed
            log.debug('Retrying as: {}'.format(url))
        else:
            break

    log.debug('Done checking link {}'.format(url))
    return res


def get_hostname(url):

    return urlparse(url)[1]


def check_url_threaded(url, errors, lock, tokens_per_host,
                       max_req_per_host, timeout):

    # Check that we don't run too many parallel checks per host
    # That could bring the host down or at least be considered
    # as the Denial of Service attack.

    # Counting parallel requests to the same host.

    # dictionaries are thread-safe,
    # but it's best to put a lock here anyway

    hostname = get_hostname(url)

    with lock:
        if hostname not in tokens_per_host:
            tokens_per_host[hostname] = max_req_per_host

    while True:
        with lock:
            if tokens_per_host[hostname] > 0:
                tokens_per_host[hostname] -= 1
                break
        time.sleep(1)

    # Do the URL check!
    res, reason = check_url(url, timeout)
    if res is False:
        with lock:
            errors.append((url, reason))

    with lock:
        tokens_per_host[hostname] += 1


def do_work(urls, errors, lock, tokens_per_host, max_req_per_host,
            timeout):

    while True:

        # Take one URL to check
        with lock:
            # Terminate the thread is the URL list is empty
            if len(urls) == 0:
                return
            url = urls.pop()

        # Ok, we have a non-excluded URL: check it!
        check_url_threaded(url, errors, lock, tokens_per_host,
                           max_req_per_host, timeout)


##########################################################
# URL extraction
##########################################################

ESC_PAT = re.compile(r'[\000-\037&<>()"\042\047\134\177-\377]')


def e(s):
    return ESC_PAT.sub(lambda m: '&#%d;' % ord(m.group(0)), s)


def is_valid(url):
    # we require that a protocol is given (i.e. the URL is absolute)
    # but we've only tested with http, https, ftp and file URLs
    protocol = urlparse(url)[0]
    return protocol


def search_url_string(obj):
    if isinstance(obj, str):
        return e(obj)
    elif isinstance(obj, bytes):
        return e(obj.decode('utf-8'))


def search_url(obj, urls):
    if obj is None:
        return

    if isinstance(obj, dict):
        for (k, v) in obj.items():

            # A dictionary with a "URI" key
            # may contain an URL string
            if k == 'URI':
                url = search_url_string(v)
                # Need to unescape html special characters
                url = html.unescape(url)
                if url is not None and is_valid(url):
                    log.debug('URL found: {}'.format(url))
                    urls.add(url)

            search_url(v, urls)

    elif isinstance(obj, list):
        for v in obj:
            search_url(v, urls)

    elif isinstance(obj, PDFStream):
        search_url(obj.attrs, urls)


def extract_urls(filename, urls):

    log.info('Checking links in file {} ...'.format(filename))

    try:
        fp = open(filename, 'rb')
        parser = PDFParser(fp)
        doc = PDFDocument(parser)
    except Exception as e:
        log.error("Cannot open file {}: {}\n".format(filename, e))
        return

    # Iterate through each object and search for URLs there
    for xref in doc.xrefs:
        for objid in xref.get_objids():
            try:
                obj = doc.getobj(objid)
                search_url(obj, urls)
            except:
                continue

    fp.close()
    return


##########################################################
# Main program
##########################################################

def main():

    # Command parameters
    # Either default values, found in a configuration file
    # or on the command line

    usage = 'usage: %prog [options] [PDF document files]'
    description = 'Reports broken hyperlinks in PDF documents'

    optparser = OptionParser(usage=usage, version='1.1.1',
                             description=description)

    optparser.add_option('-v', '--verbose',
                         action='store_true', dest='verbose', default=False,
                         help='display progress information')

    optparser.add_option('-s', '--status',
                         action='store_true', dest='status', default=False,
                         help='store check status information '
                         'in a .checked file')

    optparser.add_option('-d', '--debug',
                         action='store_true', dest='debug', default=False,
                         help='display debug information')

    optparser.add_option('-t', '--max-threads',
                         action='store', type='int',
                         dest='max_threads', default=20,
                         help='set the maximum number '
                         'of parallel threads to create')

    optparser.add_option('-r', '--max-requests-per-host',
                         action='store', type='int',
                         dest='max_requests_per_host', default=5,
                         help='set the maximum number '
                         'of parallel requests per host')

    optparser.add_option('-x', '--exclude-hosts',
                         action='store', type='string',
                         dest='exclude_hosts', default='',
                         help='ignore urls which host name '
                         'belongs to the given list')

    optparser.add_option('-p', '--include-pattern',
                         action='store', type='string',
                         dest='include_pattern', default='',
                         help='include only urls that match '
                         'the given regular expression')

    optparser.add_option('-m', '--timeout',
                         action='store', type='int',
                         dest='timeout', default=5,
                         help='set the timeout for the requests '
                         '(only used with HTTP(S))')

    optparser.add_option('', '--check-url',
                         action='store', type='string',
                         dest='check_url', default='',
                         help='checks given url instead of checking PDF (debug)')

    (options, args) = optparser.parse_args()

    if options.debug:
        level = log.DEBUG
    elif options.verbose:
        level = log.INFO
    else:
        level = log.WARNING

    log.basicConfig(stream=sys.stderr,
                    level=level,
                    format='%(levelname)s: %(message)s')

    # This option is for debug purposes only,
    # we can use it -for instance- to work out
    # a false-positive invalid URL
    if len(options.check_url) != 0:
        url = options.check_url
        res, reason = check_url(url, options.timeout)
        if res is False:
            log.error('URL {} failed. Reason: {}'.format(url, reason))
        sys.exit()

    if len(args) == 0:
        log.critical('No files to check. Exiting.')
        sys.exit()

    # Turn options.exclude_hosts into a list, for exact matching
    options.exclude_hosts = options.exclude_hosts.split()

    # Iterate on all given documents, filling urls set
    urls = set()
    for input_file in args:
        extract_urls(input_file, urls)

    if len(urls) == 0:
        log.warning('No URLs found! Exiting.')
        sys.exit()

    # Filter URLs by -p option (if given)

    if options.include_pattern:

        try:
            p = re.compile(options.include_pattern)
            urls = [url for url in urls if p.search(url)]

        except:
            log.critical('The -p argument value is not a valid regular expression. Exiting.')
            sys.exit()

        if len(urls) == 0:
            log.warning('No URLs left after -p! Exiting.')
            sys.exit()

    # Filter URLs by -x option (if given)

    if len(options.exclude_hosts) > 0:

        urls = [url for url in urls if not options.exclude_hosts.count(
            get_hostname(url))]

        if len(urls) == 0:
            log.warning('No URLs left after -x! Exiting.')
            sys.exit()

    # Run URL checks
    tokens_per_host = {}
    errors = []
    lock = threading.Lock()
    for i in range(options.max_threads):

        t = threading.Thread(target=do_work,
                             args=(urls, errors, lock,
                             tokens_per_host, options.max_requests_per_host,
                             options.timeout))
        t.start()

    # Wait for all threads to complete
    for thread in threading.enumerate():
        log.info('Waiting for URL checks '
                 'to complete: {} threads left'.
                 format(threading.active_count()-1))
        if thread is not threading.current_thread():
                thread.join()

    if len(errors) > 0:
        for url, reason in errors:
            log.error('URL {} failed. Reason: {}'.format(url, reason))
        sys.exit(1)

    if options.status:
        touch('.' + os.path.basename(input_file) + '.checked')

    log.info('Hyperlink checking successful')

    sys.exit(0)


if __name__ == "__main__":
    main()
